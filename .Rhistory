dat_copy = dat
# dat = dat_copy
# if this does not work, remove the last arguments "%d %B %Y"
dat$date <- as.Date(dat$date, format= "%d %B %Y")
#### dat = dat_copy (this is just, when you need to redo something)
## count occurence of keywords
klimawandel <- dat$body
key_count = sapply(c("Klima", "klima", "CO2", "clima", "globale Erw?rmung"),
function(i) str_count(klimawandel, i))
#### bind the results to the existing object transformed to dataframe
dat_key = as.data.frame(key_count)
dat = cbind(dat, dat_key)
# create variable that counts the number of those keywords; then create a variable that indicates if word count is greater or equal to 250.
dat = dat %>%
mutate(sum_keywords = Klima + klima + CO2 + clima+ `globale Erw?rmung`) %>%
# mutate(off_topic = ifelse(sum_keywords <= 3, 1, 0)) %>% # this variable is not necessary
mutate(above_250 = ifelse(length >= 250, 1, 0))
# mutate(above_250 = ifelse(length >= 250, 1, 0)) # also this variable is in a strict sense not necessary
# save
# FOR WINDOW USERS
# write.csv2(dat, "I:/CC_Mitigation/SNF-Project_mulitplex-climate-networks/Data/DNA_data/DE/1_Post_paris_2019-20/2019/dat_2019.csv")
# FOR MAC USERS
write.csv2(dat, "/Volumes/group-ingold/CC_Mitigation/SNF-Project_mulitplex-climate-networks/Data/DNA_data/Archiving/DE/2019/dat_2019.csv")
# make sure that you adjust the file name and file serve!!!
# To read in the file again;  dat = read.csv2("Z:/CC_Mitigation/SNF-Project_mulitplex-climate-networks/Data/Switzerland/Discourse-networks-media-data/DNA-Coding-Files/2019/dat_2019.csv")
# create subset of dat excluding off_opic variables, as well as articles above 250 words. (Different set-ups where tested and this was )
# dat_dna_2 = subset(dat, dat$sum_keywords > "2" & dat$above_250 == "1")
# dat_dna_3 = subset(dat, dat$sum_keywords > "3" & dat$above_250 == "1")
# dat_dna_4 = subset(dat, dat$sum_keywords > "4" & dat$above_250 == "1")
# dat_dna_5 = subset(dat, dat$sum_keywords > "5" & dat$above_250 == "1")
# dat_dna_6 = subset(dat, dat$sum_keywords > "6" & dat$above_250 == "1")
# dat_dna_7 = subset(dat, dat$sum_keywords > "7" & dat$above_250 == "1")
dat_dna_5_all = subset(dat, dat$sum_keywords > "5") # FOLLOWING MEETING ON 22.08 between Marlene & Jack - we proceed with the 5 keyword threshold & disregard the word count critera
# dat_dna_6_all = subset(dat, dat$sum_keywords > "6")
dat_dna = dat_dna_5_all
# I chose to take >5 and at least 250 words
# # Creating a comment card for coders
# comment.card = dat_dna %>% select(date, source, head)
# write.csv(comment.card, "DE-2019-dna_comment-card.csv")
# Determine number of articles to be drawn randomly
# This is the subset for the reliability checks - we draw a random sample of 20%
dat_dna_rel = dat_dna %>% sample_frac(.2)
# comment.card.REL = dat_dna_rel %>% select(date, source, head)
# write.csv(comment.card.REL, "DE-2019-dna_comment-card_REL.csv")
## write the relevant articles into a folder for analaysis with the DNA coding software later
for (i in 1:nrow(dat_dna)) {
title <- dat_dna$head[i]
body_text <- dat_dna$body[i]
date <- dat_dna$date[i]
source <- dat_dna$source[i]
path <- "/Volumes/group-ingold/CC_Mitigation/SNF-Project_mulitplex-climate-networks/Data/DNA_data/Archiving/DE/2019/2019_text-files/" # Please, follow the file naming conventions, strickly! NOTE THAT THE PATH IS CHANGED FOR MAC USERS
filename <- paste0(path, date, " - ", source, " - " ,  i, ".txt")
write.table(paste0(date, "\n", source, "\n", title, "\n", body_text) , file = filename,
append = TRUE, row.names = FALSE, col.names = FALSE, quote = FALSE)
}
## write the relevant articles for reliability checks into a folder for analaysis with the DNA coding software later
for (i in 1:nrow(dat_dna_rel)) {
title <- dat_dna_rel$head[i]
body_text <- dat_dna_rel$body[i]
date <- dat_dna_rel$date[i]
source <- dat_dna_rel$source[i]
path <- "/Volumes/group-ingold/CC_Mitigation/SNF-Project_mulitplex-climate-networks/Data/DNA_data/Archiving/DE/2019/2019_text-files-rel/" # Please, follow the file naming conventions, strickly!
filename <- paste0(path, date, " - ", source, " - " ,  i, ".txt")
write.table(paste0(date, "\n", source, "\n", title, "\n", body_text) , file = filename,
append = TRUE, row.names = FALSE, col.names = FALSE, quote = FALSE)
}
for (i in 1:nrow(dat_dna_rel)) {
title <- dat_dna_rel$head[i]
body_text <- dat_dna_rel$body[i]
date <- dat_dna_rel$date[i]
source <- dat_dna_rel$source[i]
path <- "/Volumes/group-ingold/CC_Mitigation/SNF-Project_mulitplex-climate-networks/Data/DNA_data/Archiving/DE/2019/2019_text-files-rel/" # Please, follow the file naming conventions, strickly!
filename <- paste0(path, date, " - ", source, " - " ,  i, ".txt")
write.table(paste0(date, "\n", source, "\n", title, "\n", body_text) , file = filename,
append = TRUE, row.names = FALSE, col.names = FALSE, quote = FALSE)
}
R.version
# platform       aarch64-apple-darwin20
# arch           aarch64
# os             darwin20
# system         aarch64, darwin20
# status
# major          4
# minor          3.2
# year           2023
# month          10
# day            31
# svn rev        85441
# language       R
# version.string R version 4.3.2 (2023-10-31)
# nickname       Eye Holes
rm(list = ls())
.rs.restartR()
setwd("/Users/simon/Documents/repo/cities-learning-dec")
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(sf)
library(showtext)
library(rnaturalearth)
library(cowplot)
library(purrr)
library(arrow)
library(ggsci)
library(stringi)
library(stringr)
library(ggpubr)
library(ggtext)
clust <- read.csv("data/clustering_results/dec_clusters_k4.csv")
ghsl <- read_sf("data/GHS_UCDB_GLOBE_R2024A_V1_0/GHS_UCDB_GLOBE_R2024A_small.gpkg")
ghsl_clean <- read_parquet("data/clustering_data_clean/GHS_UCDB_2024_preproc_2025_04_09_uci_and_nan_imputation_add_vars_included.parquet")
labelled_topics <- readxl::read_xlsx("data/topic_model/labelled_topics_2.xlsx")
main_topic <- read.csv("data/topic_model/main_topic_220.csv")
world <- ne_countries(scale = "medium", returnclass = "sf")
bb <- ne_download(type = "wgs84_bounding_box", category = "physical", returnclass = "sf")
# oa data
file_names <- list.files(
path = "/Users/simon/Documents/repo/cities-learning/data/OpenAlex/05_deduplicated",
pattern = "^city_works_df_NA_abstr_added_dedup_\\d+\\.csv$",
full.names = TRUE
)
df_list <- lapply(file_names, read.csv)
oa <- do.call(rbind, df_list)
# studies per city
clean_places <- read.csv("data/geoparser/clean_places_augmented.csv")
clean_places <- clean_places %>%
filter(!is.na(city_intersection_id) & !is.na(city_word_match_id)) %>%
filter(id %in% oa$id) %>% # only deduplicated count
mutate(city_id = ifelse(is.na(city_intersection_id), city_word_match_id, city_intersection_id)) %>%
select(id, city_id)
n_studies_per_city <- clean_places %>%
group_by(city_id) %>%
summarise(n_studies = n())
# ipcc regions
ipcc_regions <- st_read("data/IPCC-WGI-reference-regions-v4_shapefile/IPCC-WGI-reference-regions-v4.shp")
st_layers('data/IPCC-WGI-reference-regions-v4_shapefile/zones.gpkg')
ipcc_regions_hexa_groupings <- st_read('data/IPCC-WGI-reference-regions-v4_shapefile/zones.gpkg', layer = "groupings")
ipcc_regions_hexa_regionlabels <- st_read('data/IPCC-WGI-reference-regions-v4_shapefile/zones.gpkg', layer = "regionlabels")
ipcc_regions_hexa_regionals <- st_read('data/IPCC-WGI-reference-regions-v4_shapefile/zones.gpkg', layer = "regionals")
ipcc_regions_hexa_lablelines <- st_read('data/IPCC-WGI-reference-regions-v4_shapefile/zones.gpkg', layer = "labellines")
ipcc_regions_hexa_split <- st_read("data/IPCC-WGI-reference-regions-v4_shapefile/zones_hexagons_split_triangles.gpkg")
ipcc_regions_hexa <- st_read('data/IPCC-WGI-reference-regions-v4_shapefile/zones.gpkg')
cites_ipcc_regions <- read.csv("data/IPCC-WGII-continental-regions_shapefile/cities_ids_with_ipcc_regions.csv")
emmissions <- read.csv("data/emissions/balance_sheet.csv")
load_data <- function(file_name, value_col, new_name) {
read.csv(file.path("data/GHS_UCDB_GLOBE_R2024A_V1_0", file_name)) %>%
select(ID_UC_G0, all_of(value_col)) %>%
mutate(across(all_of(value_col), as.numeric)) %>%
rename(!!new_name := value_col)
}
# === Load datasets with custom names ===
gender <- load_data("socioeconomic.csv",  "SC_SEC_GDF_2020", "GHS_female_gender_index")
lecz   <- load_data("exposure.csv",       "EX_L10_B23_2020", "GHS_builtup_below_10m")
hdi    <- load_data("socioeconomic.csv",  "SC_SEC_HDI_2020", "GHS_HDI")
co_vars <- c("GHS_population", "GHS_population_growth", "GHS_population_density", "GHS_population_density_growth",
"GHS_GDP_PPP", "GHS_GDP_PPP_growth",
"GHS_critical_infra", "GHS_greenness_index", "GHS_precipitation",
"hdd", "cdd")
co_vars_formatted <- c("Population", "Population growth", "Population density", "Population density growth",
"GDP PPP", "GDP PPP growth",
"Critical infrastructure", "Greenness", "Precipitation",
"Heating degree days", "Cooling degree days")
reg_vars <- c("NORTH-AMERICA", "SOUTH-AMERICA", "EUROPE", "AFRICA", "ASIA", "OCEANIA" , "SMALL ISLANDS")
reg_vars_wg2 <- c("North America", "South America", "Europe", "Africa", "Asia", "Australasia", "Small Islands")
cluster_names <- data.frame(
consensus_label_majority = 0:3,
cluster_name = c(
"Mitigation first",
"Mega all in",
"Development first",
"Urban planning first"
)) %>%
mutate(cluster_name = factor(cluster_name, levels = c("Development first",
"Mitigation first",
"Urban planning first",
"Mega all in")))
proj_robin <- "+proj=robin"
ghsl <- st_transform(ghsl, proj_robin)
world <- st_transform(world, proj_robin)
bb <- st_transform(bb, proj_robin)
# Load and register a modern font (e.g., Helvetica Neue)
showtext_auto()  # Automatically use showtext for fonts
# Check available fonts
remotes::install_github("kjhealy/myriad")
rename_co_vars <- function(df, column) {
rename_map <- c(
"GHS_population" = "Population",
"GHS_population_growth" = "Population growth",
"GHS_population_density" = "Population density",
"GHS_population_density_growth" = "Population density growth",
"GHS_GDP_PPP" = "GDP PPP",
"GHS_GDP_PPP_growth" = "GDP PPP growth",
"GHS_critical_infra" = "Critical infrastructure",
"GHS_greenness_index" = "Greenness",
"GHS_precipitation" = "Precipitation",
"hdd" = "Heating degree days",
"cdd" = "Cooling degree days"
)
column <- rlang::ensym(column)
df %>%
mutate(!!column := recode(!!column, !!!rename_map))
}
myriad::import_myriad(font_family = "Myriad Pro", silent = F)
ipcc_regions %>%
ggplot() +
geom_sf(data = world) +
geom_sf(aes(fill = Type == "Ocean"), alpha= .3) +
geom_sf(data = ghsl %>% mutate(geom = st_centroid(geom)), aes(geometry = geom), alpha = .5, size = .5) +
geom_label(
aes(label = Acronym, geometry = st_centroid(geometry)),
stat = "sf_coordinates", alpha=.5, size = 2,
)
theme_SM <- function(){
theme_light() +
theme(panel.grid = element_blank(),
panel.border = element_rect(colour = "grey50", fill=NA, linewidth=.5),
strip.placement = "outside",
text = element_text(size = 12, family = "Myriad Pro"),
axis.text.x = element_text(colour = "grey30", angle = 45, hjust = 1, vjust = 1),
axis.text.y = element_text(colour = "grey30"),
axis.ticks.length = unit(.2, "cm"),
axis.ticks = element_line(colour = "grey50", linewidth=.5),
strip.background = element_rect(fill = "white"),
strip.text = element_text(colour = "black"),
strip.clip = "off",
legend.text = element_text(size = 7),
legend.key.size = unit(.4, "cm"),
legend.position = c(0.9,.05),
legend.margin = margin(rep(2, 4)),
legend.title = element_blank(),
legend.justification = c(1, 0),
legend.background = element_rect(fill="white",
size=.3, linetype="solid",
colour ="grey")
)
}
clust_probs <- clust %>%
pivot_longer(
cols = starts_with("mean_prob_cluster_"),
names_to = "secondary_cluster",
names_prefix = "mean_prob_cluster_",
values_to = "mean_prob"
) %>%
select(-similarity, -entropy) %>%
mutate(secondary_cluster = as.numeric(secondary_cluster)) %>%
left_join(cluster_names, by = c("secondary_cluster"="consensus_label_majority")) %>%
rename(secondary_cluster_name = cluster_name) %>%
left_join(cluster_names, by = c("consensus_label_majority"))
clust <- clust %>%
pivot_longer(
cols = starts_with("mean_prob_cluster_"),
names_to = "cluster_prob",
names_prefix = "mean_prob_cluster_",
values_to = "mean_prob"
) %>%
group_by(GHS_urban_area_id) %>%
slice_max(mean_prob) %>%
left_join(ghsl_clean, by= "GHS_urban_area_id") %>%
left_join(ghsl, by= c("GHS_urban_area_id" = "ID_UC_G0")) %>%
mutate(GHS_population = GHS_population/1000000,
GHS_population_density = GHS_population_density/1000000,
GHS_GDP_PPP = GHS_GDP_PPP/1000,
hdd = hdd,
cdd = cdd) %>%
select(GHS_urban_area_id, consensus_label_majority,
co_vars, similarity, mean_prob
) %>%
left_join(cites_ipcc_regions, by= c("GHS_urban_area_id" = "ID_UC_G0")) %>%
left_join(n_studies_per_city, by = c("GHS_urban_area_id" = "city_id")) %>%
mutate(n_studies = ifelse(is.na(n_studies), 0, n_studies)) %>%
# mutate(similarity = (similarity - min(similarity, na.rm = TRUE)) /
#          (max(similarity, na.rm = TRUE) - min(similarity, na.rm = TRUE)),
#        similarity_n_studies_per_city=similarity*n_studies
#        ) %>%
group_by(consensus_label_majority, Region) %>%
arrange(consensus_label_majority, Region, -mean_prob) %>%
mutate(representative_city = row_number()<=2)
sort(unique(clust$consensus_label_majority))
ggplot(ipcc_regions_hexa) +
geom_sf(fill = NA) +
geom_label(
aes(label = label, geometry = geom),
stat = "sf_coordinates", alpha=.5, size = 2,
)
################################################################################
# quick look at the data
################################################################################
ggplot(ipcc_regions_hexa) +
geom_sf() +
geom_label(
aes(label = label, geometry = geom),
stat = "sf_coordinates", alpha=.5, size = 2,
)
ipcc_regions <- ipcc_regions %>%
mutate(Acronym = ifelse(Type == "Ocean", "PAC", Acronym))
ipcc_regions %>%
st_join(ghsl %>% mutate(geom = st_centroid(geom)) %>% st_transform(4326) %>% select("ID_UC_G0")) %>%
filter(!is.na(ID_UC_G0)) %>%
select(-ID_UC_G0) %>%
group_by(Acronym) %>%
slice(1) %>%
# mutate(Acronym = ifelse(Type == "Ocean", "PAC", Acronym))
ggplot() +
geom_sf(aes(fill = Type == "Ocean"), alpha= .3) +
geom_label(
aes(label = Acronym, geometry = st_centroid(geometry)),
stat = "sf_coordinates", alpha=.5, size = 2,
) +
geom_sf(data = ghsl %>% mutate(geom = st_centroid(geom)), aes(geometry = geom))
################################################################################
# figures
################################################################################
desc_dat <- clust %>%
filter(!is.na(consensus_label_majority)) %>%
as.data.frame() %>%
select(consensus_label_majority, co_vars,
Region, mean_prob,
GHS_urban_area_id)
# Compute median emissions per Region Ã— cluster_name
emmissions_dat <- emmissions %>%
as.data.frame() %>%
left_join(
clust %>% select(consensus_label_majority, Region, GHS_urban_area_id, GHS_population),
by = c("ID_UC_G0" = "GHS_urban_area_id")
) %>%
left_join(cluster_names, by = "consensus_label_majority") %>%
left_join(ghsl %>% select(GC_POP_TOT_2025, ID_UC_G0), by = c("ID_UC_G0")) %>%
mutate(
odiac_norm = ODIAC / GC_POP_TOT_2025,
Region = factor(Region, levels = reg_vars)
)
emmissions_box_dat <- emmissions_dat %>%
filter(Year == 2022)
# Join medians back
emmissions_box_dat <- emmissions_box_dat %>%
left_join(median_data, by = c("Region", "cluster_name")) %>%
group_by(Region, cluster_name) %>%
mutate(
Q1 = quantile(odiac_norm, 0.25, na.rm = TRUE),
Q3 = quantile(odiac_norm, 0.75, na.rm = TRUE),
IQR_val = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR_val,
upper_bound = Q3 + 1.5 * IQR_val
) %>%
filter(odiac_norm >= lower_bound & odiac_norm <= upper_bound) %>%
ungroup()
# --- 2. Create interpolation grid based on data extent ---
res <- 50000  # resolution in meters
bb <- bbox(ghsl_df)
library(sf)
library(gstat)
library(raster)
library(ggplot2)
library(terra)
# Extract coordinates and create SpatialPointsDataFrame
ghsl_df <- ghsl %>%
st_transform(proj_robin) %>%
mutate(geom = st_centroid(geom)) %>%
left_join(learn_pot %>% dplyr::select(ID_UC_G0, learning_volume_potential), by = "ID_UC_G0") %>%
mutate(
x = st_coordinates(geom)[, 1],
y = st_coordinates(geom)[, 2]
) %>%
st_drop_geometry()  # Drop sf geometry to make a pure data.frame
learn_pot <- co_mat %>%
left_join(ghsl %>% dplyr::select(ID_UC_G0, GC_UCN_MAI_2025, GC_CNT_GAD_2025), by = "ID_UC_G0") %>%
dplyr::select(ID_UC_G0, consensus_label_majority, similarity_decile, research_volume, research_evenness,
learning_volume_potential, learning_diversity_potential, teaching_volume_potential,
# learning_index, teaching_index
) %>%
group_by(consensus_label_majority) %>%
arrange(desc(learning_volume_potential))
# Step 1: Subset cities in cluster 1
cluster_cities <- ghsl %>%
mutate(geom = st_centroid(geom)) %>%
dplyr::select(ID_UC_G0, geom) %>%
left_join(clust_with_topics, by = c("ID_UC_G0" = "GHS_urban_area_id"))
R.version
# platform       aarch64-apple-darwin20
# arch           aarch64
# os             darwin20
# system         aarch64, darwin20
# status
# major          4
# minor          3.2
# year           2023
# month          10
# day            31
# svn rev        85441
# language       R
# version.string R version 4.3.2 (2023-10-31)
# nickname       Eye Holes
rm(list = ls())
.rs.restartR()
setwd("/Users/simon/Documents/repo/cities-learning-dec")
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(sf)
library(showtext)
library(rnaturalearth)
library(cowplot)
library(purrr)
library(arrow)
library(ggsci)
library(stringi)
library(stringr)
library(ggpubr)
library(ggtext)
################################################################################
# load data
################################################################################
clust <- read.csv("data/clustering_results/dec_clusters_k4.csv")
# status
# major          4
# minor          3.2
# year           2023
# month          10
# day            31
# svn rev        85441
# language       R
# version.string R version 4.3.2 (2023-10-31)
# nickname       Eye Holes
rm(list = ls())
setwd("/Users/simon/Documents/repo/cities-learning-dec")
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(sf)
library(showtext)
library(rnaturalearth)
library(cowplot)
library(purrr)
library(arrow)
library(ggsci)
library(stringi)
library(stringr)
library(ggpubr)
library(ggtext)
clust <- read.csv("data/clustering_results/dec_clusters_k4.csv")
ghsl <- read_sf("data/GHS_UCDB_GLOBE_R2024A_V1_0/GHS_UCDB_GLOBE_R2024A_small.gpkg")
ghsl_clean <- read_parquet("data/clustering_data_clean/GHS_UCDB_2024_preproc_2025_04_09_uci_and_nan_imputation_add_vars_included.parquet")
labelled_topics <- readxl::read_xlsx("data/topic_model/labelled_topics_2.xlsx")
main_topic <- read.csv("data/topic_model/main_topic_220.csv")
world <- ne_countries(scale = "medium", returnclass = "sf")
bb <- ne_download(type = "wgs84_bounding_box", category = "physical", returnclass = "sf")
# oa data
file_names <- list.files(
path = "/Users/simon/Documents/repo/cities-learning/data/OpenAlex/05_deduplicated",
pattern = "^city_works_df_NA_abstr_added_dedup_\\d+\\.csv$",
full.names = TRUE
)
R.version
# platform       aarch64-apple-darwin20
# arch           aarch64
# os             darwin20
# system         aarch64, darwin20
# status
# major          4
# minor          3.2
# year           2023
# month          10
# day            31
# svn rev        85441
# language       R
# version.string R version 4.3.2 (2023-10-31)
# nickname       Eye Holes
rm(list = ls())
setwd("/Users/simon/Documents/repo/cities-learning-dec")
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(sf)
library(showtext)
library(rnaturalearth)
library(cowplot)
library(purrr)
library(arrow)
library(ggsci)
library(stringi)
library(stringr)
library(ggpubr)
library(ggtext)
################################################################################
# load data
################################################################################
clust <- read.csv("data/clustering_results/dec_clusters_k4.csv")
ghsl <- read_sf("data/GHS_UCDB_GLOBE_R2024A_V1_0/GHS_UCDB_GLOBE_R2024A_small.gpkg")
ghsl_clean <- read_parquet("data/clustering_data_clean/GHS_UCDB_2024_preproc_2025_04_09_uci_and_nan_imputation_add_vars_included.parquet")
labelled_topics <- readxl::read_xlsx("data/topic_model/labelled_topics_2.xlsx")
main_topic <- read.csv("data/topic_model/main_topic_220.csv")
world <- ne_countries(scale = "medium", returnclass = "sf")
bb <- ne_download(type = "wgs84_bounding_box", category = "physical", returnclass = "sf")
# oa data
file_names <- list.files(
path = "/Users/simon/Documents/repo/cities-learning/data/OpenAlex/05_deduplicated",
pattern = "^city_works_df_NA_abstr_added_dedup_\\d+\\.csv$",
full.names = TRUE
)
df_list <- lapply(file_names, read.csv)
oa <- do.call(rbind, df_list)
# studies per city
clean_places <- read.csv("data/geoparser/clean_places_augmented.csv")
clean_places <- clean_places %>%
filter(!is.na(city_intersection_id) & !is.na(city_word_match_id)) %>%
filter(id %in% oa$id) %>% # only deduplicated count
mutate(city_id = ifelse(is.na(city_intersection_id), city_word_match_id, city_intersection_id)) %>%
select(id, city_id)
