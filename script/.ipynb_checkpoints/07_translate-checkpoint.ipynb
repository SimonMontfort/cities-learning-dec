{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c7830-0c58-47e0-87c0-f30c343f30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from langdetect import detect\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "os.chdir(\"/Users/simon/Documents/repo/cities-learning\")\n",
    "\n",
    "# -----------------------\n",
    "# 1. Combine feather files\n",
    "# -----------------------\n",
    "def combine_files(directory, pattern):\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "    print(f\"Found {len(files)} files.\")\n",
    "    dfs = [pd.read_feather(f) for f in files]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# 2. Clean dataset\n",
    "# -----------------------\n",
    "def clean_data(df):\n",
    "    print(f\"initial number of observations: {len(df)}\")\n",
    "    df = df.drop_duplicates(subset=\"id\")\n",
    "    print(f\"observations after dropping exact duplicates: {len(df)}\")\n",
    "\n",
    "    df = df[~df[\"title\"].str.contains(\".xlsx\", na=False)]\n",
    "    exclude_types = [\"dataset\", \"erratum\", \"retraction\", \"peer-review\", \"reference-entry\"]\n",
    "    df = df[~df[\"type\"].isin(exclude_types)]\n",
    "\n",
    "    print(f\"observations after dropping the following types {exclude_types}: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# 3. Sentence splitting\n",
    "# -----------------------\n",
    "def split_sentences(text):\n",
    "    return re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "\n",
    "# -----------------------\n",
    "# 4. Language detection\n",
    "# -----------------------\n",
    "def detect_language_safe(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# -----------------------\n",
    "# 5. Load NLLB translator\n",
    "# -----------------------\n",
    "nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "nllb_tokenizer = AutoTokenizer.from_pretrained(nllb_model_name)\n",
    "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name)\n",
    "nllb_model = nllb_model.to(\"cpu\")  # Ensure model runs on CPU to avoid GPU memory issues\n",
    "\n",
    "# Full mapping from langdetect codes to NLLB language codes\n",
    "langdetect_to_nllb = {\n",
    "    \"af\": \"afr_Latn\", \"am\": \"amh_Ethi\", \"ar\": \"arb_Arab\", \"az\": \"azj_Latn\", \"be\": \"bel_Cyrl\", \"bg\": \"bul_Cyrl\",\n",
    "    \"bn\": \"ben_Beng\", \"bs\": \"bos_Latn\", \"ca\": \"cat_Latn\", \"ceb\": \"ceb_Latn\", \"cs\": \"ces_Latn\", \"cy\": \"cym_Latn\",\n",
    "    \"da\": \"dan_Latn\", \"de\": \"deu_Latn\", \"el\": \"ell_Grek\", \"en\": \"eng_Latn\", \"es\": \"spa_Latn\", \"et\": \"est_Latn\",\n",
    "    \"eu\": \"eus_Latn\", \"fa\": \"pes_Arab\", \"fi\": \"fin_Latn\", \"fr\": \"fra_Latn\", \"gl\": \"glg_Latn\", \"gu\": \"guj_Gujr\",\n",
    "    \"ha\": \"hau_Latn\", \"hi\": \"hin_Deva\", \"hr\": \"hrv_Latn\", \"ht\": \"hat_Latn\", \"hu\": \"hun_Latn\", \"hy\": \"hye_Armn\",\n",
    "    \"id\": \"ind_Latn\", \"ig\": \"ibo_Latn\", \"is\": \"isl_Latn\", \"it\": \"ita_Latn\", \"iw\": \"heb_Hebr\", \"ja\": \"jpn_Jpan\",\n",
    "    \"jv\": \"jav_Latn\", \"ka\": \"kat_Geor\", \"kk\": \"kaz_Cyrl\", \"km\": \"khm_Khmr\", \"kn\": \"kan_Knda\", \"ko\": \"kor_Hang\",\n",
    "    \"lo\": \"lao_Laoo\", \"lt\": \"lit_Latn\", \"lv\": \"lvs_Latn\", \"mg\": \"plt_Latn\", \"mi\": \"mri_Latn\", \"mk\": \"mkd_Cyrl\",\n",
    "    \"ml\": \"mal_Mlym\", \"mn\": \"khk_Cyrl\", \"mr\": \"mar_Deva\", \"ms\": \"zsm_Latn\", \"mt\": \"mlt_Latn\", \"my\": \"mya_Mymr\",\n",
    "    \"ne\": \"npi_Deva\", \"nl\": \"nld_Latn\", \"no\": \"nob_Latn\", \"pa\": \"pan_Guru\", \"pl\": \"pol_Latn\", \"ps\": \"pbt_Arab\",\n",
    "    \"pt\": \"por_Latn\", \"ro\": \"ron_Latn\", \"ru\": \"rus_Cyrl\", \"si\": \"sin_Sinh\", \"sk\": \"slk_Latn\", \"sl\": \"slv_Latn\",\n",
    "    \"so\": \"som_Latn\", \"sq\": \"als_Latn\", \"sr\": \"srp_Cyrl\", \"su\": \"sun_Latn\", \"sv\": \"swe_Latn\", \"sw\": \"swh_Latn\",\n",
    "    \"ta\": \"tam_Taml\", \"te\": \"tel_Telu\", \"th\": \"tha_Thai\", \"tl\": \"tgl_Latn\", \"tr\": \"tur_Latn\", \"uk\": \"ukr_Cyrl\",\n",
    "    \"ur\": \"urd_Arab\", \"uz\": \"uzn_Latn\", \"vi\": \"vie_Latn\", \"xh\": \"xho_Latn\", \"yi\": \"ydd_Hebr\", \"zh-cn\": \"zho_Hans\",\n",
    "    \"zh-tw\": \"zho_Hant\", \"zu\": \"zul_Latn\", \"unknown\": \"eng_Latn\"\n",
    "}\n",
    "\n",
    "def translate_sentence_nllb(text, src_lang=\"eng_Latn\", tgt_lang=\"eng_Latn\"):\n",
    "    try:\n",
    "        inputs = nllb_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs[\"forced_bos_token_id\"] = nllb_tokenizer.lang_code_to_id[tgt_lang]\n",
    "        translated = nllb_model.generate(**inputs)\n",
    "        return nllb_tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return text\n",
    "\n",
    "# -----------------------\n",
    "# 6. Row processing logic\n",
    "# -----------------------\n",
    "def process_row(i, clim_deb_it, variable):\n",
    "    result = {\"translated\": \"\", \"lang\": \"\", \"filt\": \"\"}\n",
    "    \n",
    "    try:\n",
    "        text = clim_deb_it.iloc[i][variable]\n",
    "        if not pd.isna(text):\n",
    "            text_sentences = split_sentences(text)\n",
    "            text_languages = [detect_language_safe(s) for s in text_sentences]\n",
    "\n",
    "            translated_sentences = []\n",
    "            for s, lang in zip(text_sentences, text_languages):\n",
    "                lang_code = langdetect_to_nllb.get(lang.lower(), \"eng_Latn\")\n",
    "                if lang_code != \"eng_Latn\":\n",
    "                    translated = translate_sentence_nllb(s, src_lang=lang_code)\n",
    "                    translated_sentences.append(translated)\n",
    "                else:\n",
    "                    translated_sentences.append(s)\n",
    "\n",
    "            result[\"translated\"] = \" \".join(translated_sentences)\n",
    "            result[\"lang\"] = \", \".join(set(text_languages))\n",
    "            result[\"filt\"] = \"translated\" if any(lang != \"en\" for lang in text_languages) else \"text was already in english\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# -----------------------\n",
    "# 7. Apply translation in parallel (limited to 5 cores)\n",
    "# -----------------------\n",
    "def translate_dataframe(df, variable=\"abstract\", n_jobs=5):\n",
    "    tqdm_bar = tqdm(range(len(df)), desc=\"Processing rows\")\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_row)(i, df, variable) for i in tqdm_bar\n",
    "    )\n",
    "\n",
    "    translated = [r[\"translated\"] for r in results]\n",
    "    languages = [r[\"lang\"] for r in results]\n",
    "    filtering = [r[\"filt\"] for r in results]\n",
    "\n",
    "    df[f\"{variable}_en\"] = translated\n",
    "    df[f\"{variable}_languages\"] = languages\n",
    "    df[f\"{variable}_language_filtering\"] = filtering\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# 8. Split and write\n",
    "# -----------------------\n",
    "def split_dataframe(df, chunk_size):\n",
    "    return [df.iloc[i:i + chunk_size].copy() for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "def write_chunks(dfs, base_filename, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.reset_index(drop=True).to_feather(f\"{output_dir}/{base_filename}_{i+1}.feather\")\n",
    "\n",
    "# -----------------------\n",
    "# 9. Full pipeline\n",
    "# -----------------------\n",
    "def run_translation_pipeline(    \n",
    "    input_dir = \"data/OpenAlex/02_NA_added\",\n",
    "    output_dir = \"data/OpenAlex/03_translated\",\n",
    "    pattern = \"city_works_df_NA_abstr_added_*.feather\",\n",
    "    base_filename = \"city_works_df_translated\",\n",
    "    var = \"abstract\"):\n",
    "\n",
    "    print(\"Combining input files...\")\n",
    "    data = combine_files(input_dir, pattern)\n",
    "\n",
    "    print(\"Cleaning data...\")\n",
    "    data = clean_data(data)\n",
    "\n",
    "    print(\"Splitting data...\")\n",
    "    chunks = split_dataframe(data, chunk_size=100)\n",
    "\n",
    "    results = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Translating chunk {i+1}/{len(chunks)}\")\n",
    "        translated_chunk = translate_dataframe(chunk, variable=var, n_jobs=2)\n",
    "        results.append(translated_chunk)\n",
    "        break  # Remove to process all chunks\n",
    "\n",
    "    print(\"Saving results...\")\n",
    "    write_chunks(results, base_filename, output_dir)\n",
    "    print(\"Translation complete.\")\n",
    "\n",
    "# -----------------------\n",
    "# 10. Run the script\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_translation_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fd05a12-70bc-4759-8280-84abdf8f8c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translated': 'The most important anthropogenic influences on climate are the emission of greenhouse gases1 and changes in land use, such as urbanization and agriculture2. But it has been difficult to separate these two influences because both tend to increase the daily mean surface temperature3,4. The impact of urbanization has been estimated by comparing observations in cities with those in surrounding rural areas, but the results differ significantly depending on whether population data5 or satellite measurements of night light6-8 are used to classify urban and rural areas7,8. Here we use the difference between trends in observed surface temperatures in the continental United States and the corresponding trends in a reconstruction of surface temperatures determined from a reanalysis of global weather over the past 50 years, which is insensitive to surface observations, to estimate the impact of land-use changes on surface warming. Our results suggest that half of the observed decrease in diurnal temperature range is due to urban and other land-use changes. Moreover, our estimate of 0.27°C mean surface warming per century due to land-use changes is at least twice as high as previous estimates based on urbanization alone7,8.', 'lang': 'en', 'filt': 'text was already in english'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_feather(\"data/OpenAlex/02_NA_added/city_works_df_NA_abstr_added_1.feather\")\n",
    "result = process_row(0, df, \"abstract\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
