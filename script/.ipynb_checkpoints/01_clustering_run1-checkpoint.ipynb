{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901befdb-3276-464e-883e-1a687982f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from torch.optim import SGD\n",
    "import geopandas as gpd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir('/Users/simon/Documents/repo/cities-learning')\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a43105",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_clean = pd.read_parquet('data/clustering_data_clean/GHS_UCDB_2024_preproc_2025_04_09_uci_and_nan_imputation.parquet', engine='pyarrow')\n",
    "ghsl = gpd.read_file(\"data/GHS_UCDB_GLOBE_R2024A_V1_0/GHS_UCDB_GLOBE_R2024A_small.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79430ba2-4af7-41a2-881d-1be4e5f1630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge precipitation to the clean data\n",
    "climate_df = pd.read_csv(\"data/GHS_UCDB_GLOBE_R2024A_V1_0/GHS_UCDB_THEME_CLIMATE_GLOBE_R2024A.csv\")\n",
    "climate_df = climate_df[['ID_UC_G0', 'CL_B12_CUR_2010']]\n",
    "cities_clean = cities_clean.merge(climate_df, left_on='GHS_urban_area_id', right_on='ID_UC_G0', how='left')\n",
    "# Drop the duplicate column if you don't need both IDs\n",
    "cities_clean.drop(columns='ID_UC_G0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba862eb-a4da-47bb-a57c-7662f86bb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load world boundaries\n",
    "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04706ce7-032b-4fd6-b7bc-47faaf60c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cities_clean.columns.values.tolist())\n",
    "\n",
    "print(set(cities_clean['continent']))\n",
    "\n",
    "cities_clean['North America'] = (cities_clean['continent'] == 'North America')*1\n",
    "cities_clean['South America'] = (cities_clean['continent'] == 'South America')*1\n",
    "cities_clean['Europe'] = (cities_clean['continent'] == 'Europe')*1\n",
    "cities_clean['Africa'] = (cities_clean['continent'] == 'Africa')*1\n",
    "cities_clean['Asia'] = (cities_clean['continent'] == 'Asia')*1\n",
    "cities_clean['Oceania'] = (cities_clean['continent'].isin(['Oceania','Australia']))*1\n",
    "\n",
    "print(cities_clean['Oceania'])\n",
    "\n",
    "\n",
    "\n",
    "# Define the variables for clustering\n",
    "variables = ['GHS_population', 'GHS_population_growth', \n",
    "             'GHS_population_density', 'GHS_population_density_growth', \n",
    "             'GHS_GDP_PPP', 'GHS_GDP_PPP_growth', \n",
    "             # 'GHS_built_up_area_pc', 'GHS_built_up_area_pc_growth', \n",
    "             # 'GHS_urban_sprawl', \n",
    "             \"CL_B12_CUR_2010\",\n",
    "             'hdd', 'cdd' \n",
    "\n",
    "             # 'Africa', 'Asia', 'Europe', 'North America', 'Oceania', 'South America'\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d366c-a501-440c-a6e1-f9caf2e64c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_clean_sub = cities_clean[variables + ['GHS_urban_area_id']].copy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "missing_values = cities_clean_sub.apply(pd.to_numeric, errors='coerce').isna().sum()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "ax = sns.barplot(x=missing_values.index, y=missing_values.values, palette=\"viridis\")\n",
    "\n",
    "# Formatting\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels\n",
    "plt.ylabel(\"Missing Values Count\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.title(\"Missing Values Per Column in Dataset\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cf8f9-27df-44ad-835d-232ba38f1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking for NaN values...\")\n",
    "#columns with NaN values\n",
    "nan_columns = cities_clean_sub.isna().sum()\n",
    "print(\"NaN counts per column:\")\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with infinite values\n",
    "inf_columns = (cities_clean_sub == np.inf).sum() + (cities_clean_sub == -np.inf).sum()\n",
    "print(\"Infinite values per column:\")\n",
    "print(inf_columns[inf_columns > 0])\n",
    "\n",
    "# rows with infinite values per column\n",
    "for column in inf_columns[inf_columns > 0].index:\n",
    "    inf_rows_in_column = cities_clean_sub[cities_clean_sub[column].isin([np.inf, -np.inf])]\n",
    "    print(f\"Rows with infinite values in column '{column}' (IDs):\")\n",
    "    print(inf_rows_in_column['GHS_urban_area_id'].unique())  # Ensures unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f82c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # mean of 0, se. of 1\n",
    "\n",
    "cities_clean_scaled = scaler.fit_transform(cities_clean_sub[variables])\n",
    "\n",
    "# Display scaled data summary\n",
    "print(pd.DataFrame(cities_clean_scaled).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8dc4e-ba3f-4c9c-88a6-1d907d64bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.cluster import KMeans\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Define the DEC model architecture\n",
    "def build_model(hp):\n",
    "    input_dim = cities_clean_scaled.shape[1]\n",
    "    latent_dim_max = min(10, input_dim - 1)\n",
    "    latent_dim = hp.Int('latent_dim', min_value=2, max_value=latent_dim_max)\n",
    "    \n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(hp.Int('encoder_1_units', min_value=32, max_value=128, step=16), activation='relu')(input_layer)\n",
    "    encoder = Dense(hp.Int('encoder_2_units', min_value=32, max_value=128, step=16), activation='relu')(encoder)\n",
    "    encoder = Dense(hp.Int('encoder_3_units', min_value=32, max_value=128, step=16), activation='relu')(encoder)\n",
    "    latent_space = Dense(latent_dim, activation='relu')(encoder)\n",
    "\n",
    "    decoder = Dense(hp.Int('decoder_1_units', min_value=32, max_value=128, step=16), activation='relu')(latent_space)\n",
    "    decoder = Dense(hp.Int('decoder_2_units', min_value=32, max_value=128, step=16), activation='relu')(decoder)\n",
    "    decoder = Dense(hp.Int('decoder_3_units', min_value=32, max_value=128, step=16), activation='relu')(decoder)\n",
    "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model with hyperparameter tunable learning rate\n",
    "    model.compile(optimizer=SGD(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-1, sampling='log')), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=200,\n",
    "    factor=3,\n",
    "    directory='hyperband',\n",
    "    project_name='DEC_model_tuning_run1'\n",
    ")\n",
    "\n",
    "# Set the search space for the hyperparameters and start the search\n",
    "tuner.search(cities_clean_scaled, cities_clean_scaled, epochs=200, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Get the best model after hyperparameter tuning\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ac5a7-0b99-46a6-996c-677cf97d319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2dbf4-c54b-47b3-b63d-16679a618063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model.summary()\n",
    "\n",
    "best_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48287b7b-ce38-492c-aea0-19b1098bf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "silhouette_scores = {\n",
    "        'DEC': [],\n",
    "        'KMeans': [],\n",
    "        'Hierarchical': []\n",
    "    }\n",
    "min_clusters=3\n",
    "max_clusters=50\n",
    "cluster_range = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "data = cities_clean_scaled\n",
    "# Get the encoded data for DEC\n",
    "encoded_data = best_model.predict(data)\n",
    "\n",
    "for n_clusters in cluster_range:\n",
    "    # DEC clustering\n",
    "    kmeans_dec = KMeans(n_clusters=n_clusters, n_init=20, random_state=42)\n",
    "    y_pred_dec = kmeans_dec.fit_predict(encoded_data)\n",
    "    sil_score_dec = silhouette_score(encoded_data, y_pred_dec)\n",
    "    silhouette_scores['DEC'].append(sil_score_dec)\n",
    "        \n",
    "    # K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=42)\n",
    "    y_pred_kmeans = kmeans.fit_predict(cities_clean_scaled)    \n",
    "    sil_score_kmeans = silhouette_score(cities_clean_scaled, y_pred_kmeans)\n",
    "    silhouette_scores['KMeans'].append(sil_score_kmeans)\n",
    "        \n",
    "    # Hierarchical clustering\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    y_pred_hierarchical = hierarchical.fit_predict(cities_clean_scaled)\n",
    "    sil_score_hierarchical = silhouette_score(cities_clean_scaled, y_pred_hierarchical)\n",
    "    silhouette_scores['Hierarchical'].append(sil_score_hierarchical)\n",
    "    \n",
    "    print(f\"scores computed for {n_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2ca6b-6947-4e42-aaa8-5abb835f8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the silhouette scores\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(cluster_range, silhouette_scores['DEC'], marker='o', label='DEC')\n",
    "plt.plot(cluster_range, silhouette_scores['KMeans'], marker='s', label='KMeans')\n",
    "plt.plot(cluster_range, silhouette_scores['Hierarchical'], marker='^', label='Hierarchical')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for Different Clustering Methods')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e8d4e-941e-4091-90b6-304a5b2f577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "silhouette_scores = pd.DataFrame(silhouette_scores)\n",
    "silhouette_scores[\"cluster_range\"] = list(range(min_clusters, max_clusters + 1))\n",
    "\n",
    "# Take a peek\n",
    "print(silhouette_scores.head())\n",
    "\n",
    "silhouette_scores.to_csv(\"data/clustering_results/silhouette_scores_run1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5f4b9-018d-48a2-a2e2-2552779c91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List to store the column names\n",
    "col_names = []\n",
    "\n",
    "# Loop through different cluster sizes\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "    y_pred = kmeans.fit_predict(encoded_data)\n",
    "    y_pred = y_pred +1\n",
    "    \n",
    "    print(k)\n",
    "    \n",
    "    # Ensure k is a string in the desired format\n",
    "    if k <= 9:\n",
    "        k_str = \"0\" + str(k)  # Format as \"03\", \"04\", etc.\n",
    "    else:\n",
    "        k_str = str(k)  # Format as \"10\", \"11\", etc.\n",
    "        \n",
    "    # Also add it to the cities_clean_sub DataFrame\n",
    "    column_name = \"y_pred_dec_\" + k_str\n",
    "    cities_clean_sub[column_name] = y_pred\n",
    "\n",
    "    print(f\"Clustering done for k={k}, stored as {column_name}\")\n",
    "    \n",
    "    # Add the column name to the list for sorting later\n",
    "    col_names.append(column_name)\n",
    "\n",
    "print(cities_clean_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769cec2-1535-429b-8c31-5778a93679e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns in the DataFrame\n",
    "cities_clean_sub[['y_pred_dec_' + str(k).zfill(2) for k in cluster_range]] = cities_clean_sub[['y_pred_dec_' + str(k).zfill(2) for k in cluster_range]]\n",
    "\n",
    "print(cities_clean_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3654414-4b9f-498d-9ba4-37c1cffee474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Initialize a dictionary to store the most representative cities_clean in each cluster\n",
    "representative_cities_clean = {}\n",
    "\n",
    "n_representatives = 5  # Specify the number of representative cities_clean per cluster\n",
    "n_clusters = 11\n",
    "\n",
    "# Calculate the most representative cities_clean in each cluster\n",
    "for cluster in range(1, n_clusters+1):\n",
    "    cluster_indices = np.where(cities_clean_sub[['y_pred_dec_' + str(n_clusters).zfill(2)]] == cluster)[0]\n",
    "    cluster_data = cities_clean_scaled[cluster_indices]\n",
    "    \n",
    "    print(len(cluster_data))\n",
    "\n",
    "    # Calculate pairwise distances within the cluster\n",
    "    distances = pairwise_distances(cluster_data)\n",
    "    \n",
    "    # Compute the average distance for each city to all other cities_clean in the cluster\n",
    "    avg_distances = distances.mean(axis=1)\n",
    "    \n",
    "    # Find the indices of the n_representatives cities_clean with the smallest average distance\n",
    "    most_representative_indices = cluster_indices[np.argsort(avg_distances)[:n_representatives]]\n",
    "\n",
    "    # print(most_representative_indices)\n",
    "    \n",
    "    # Add the most representative cities_clean to the dictionary\n",
    "    for i, index in enumerate(most_representative_indices):\n",
    "        representative_city = cities_clean.iloc[index].copy()\n",
    "        representative_city['Cluster'] = cluster\n",
    "        representative_city['GHS_urban_area_id'] = cities_clean.iloc[index]['GHS_urban_area_id']\n",
    "        # Use a unique key for each representative city\n",
    "        key = f\"{cluster}_{i+1}\"\n",
    "        representative_cities_clean[key] = representative_city\n",
    "    \n",
    "        # print(\"Cluster representative city id:\", cities_clean.iloc[index]['GHS_urban_area_id'])\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better readability\n",
    "representative_cities_clean_df = pd.DataFrame.from_dict(representative_cities_clean, orient='index')\n",
    "\n",
    "representative_cities_clean_df[\"representative_city\"] =  \"representative\"\n",
    "\n",
    "print(representative_cities_clean_df[[\"representative_city\", \"GHS_urban_area_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0ce4b-edf1-42ca-b17b-ef462a1c2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_cities_clean_df['GHS_urban_area_id'] = representative_cities_clean_df['GHS_urban_area_id'].astype(int)\n",
    "ghsl['ID_UC_G0'] = ghsl['ID_UC_G0'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78ce2d-00c6-4e04-bce3-001d223b729e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now perform a left join with the original cities_clean_df on 'GHS_urban_area_id'\n",
    "cities_clean_sub_rep = cities_clean_sub.merge(\n",
    "    representative_cities_clean_df[[\"representative_city\", \"GHS_urban_area_id\"]], \n",
    "    on = 'GHS_urban_area_id', how = \"left\")\n",
    "\n",
    "cities_clean_sub_rep = cities_clean_sub_rep.merge(\n",
    "    cities_clean[['Africa', 'Asia', 'Europe', 'North America', 'Oceania', 'South America', 'GHS_urban_area_id']], \n",
    "    on = 'GHS_urban_area_id', how = \"left\")\n",
    "\n",
    "cities_clean_sub_rep.loc[pd.isna(cities_clean_sub_rep[\"representative_city\"]), \"representative_city\"] = \"non-representative\"\n",
    "\n",
    "# Print the DataFrame of representative cities_clean\n",
    "print(cities_clean_sub_rep[['GHS_urban_area_id', 'representative_city', \"y_pred_dec_10\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4644f20-ea86-4289-8f11-7de36c949f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort column names numerically\n",
    "# col_names_sorted = sorted(col_names, key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "# Save the df\n",
    "filename = 'data/clustering_results/clustering_results_run1.csv'\n",
    "cities_clean_sub_rep.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6381385-1300-45f7-b9f2-3ec69d411894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
