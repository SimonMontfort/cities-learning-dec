{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d676d3d4-5247-4d52-8b9f-4da098565599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import glob\n",
    "\n",
    "dir_path = \"/Users/simon/Documents/repo/cities-learning\"\n",
    "\n",
    "# Create the full search pattern\n",
    "file_pattern = os.path.join(dir_path, \"data/OpenAlex/02_NA_added/city_works_df_NA_abstr_added_*.feather\")\n",
    "\n",
    "# Find all matching files\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read all the files into a list of DataFrames\n",
    "dfs = [pd.read_feather(file) for file in files]\n",
    "\n",
    "# Optionally, concatenate them into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12946594-b645-4b78-b14a-1a96976ef1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(combined_df.columns)\\n\\ncombined_df[\"abstract\"][combined_df[\"abstract_filtering\"] == \"no translatable lang\"]\\nprint(combined_df[\"abstract_filtering\"].unique())\\nprint(f\"abstracts entirely english: \" + str(sum(combined_df[\"abstract_filtering\"] == \"already english\")))\\nprint(f\"abstracts mostly english: \" + str(sum(combined_df[\"abstract_filtering\"] == \"mostly english\")))\\nprint(f\"abstracts filtered english part: \" + str(sum(combined_df[\"abstract_filtering\"] == \"filtered english\")))\\nprint(f\"abstracts translated: \" + str(sum(combined_df[\"abstract_filtering\"] == \"translated full text\")))\\nprint(f\"abstracts with no translatable lang: \" + str(sum(combined_df[\"abstract_filtering\"] == \"no translatable lang\")))\\nprint(f\"abstracts unknown: \" + str(sum(combined_df[\"abstract_filtering\"] == \"unknown\")))\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(combined_df.columns)\n",
    "\n",
    "combined_df[\"abstract\"][combined_df[\"abstract_filtering\"] == \"no translatable lang\"]\n",
    "print(combined_df[\"abstract_filtering\"].unique())\n",
    "print(f\"abstracts entirely english: \" + str(sum(combined_df[\"abstract_filtering\"] == \"already english\")))\n",
    "print(f\"abstracts mostly english: \" + str(sum(combined_df[\"abstract_filtering\"] == \"mostly english\")))\n",
    "print(f\"abstracts filtered english part: \" + str(sum(combined_df[\"abstract_filtering\"] == \"filtered english\")))\n",
    "print(f\"abstracts translated: \" + str(sum(combined_df[\"abstract_filtering\"] == \"translated full text\")))\n",
    "print(f\"abstracts with no translatable lang: \" + str(sum(combined_df[\"abstract_filtering\"] == \"no translatable lang\")))\n",
    "print(f\"abstracts unknown: \" + str(sum(combined_df[\"abstract_filtering\"] == \"unknown\")))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5d42965-1d33-4055-8fd6-e433bba01bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations before exact deduplication:\n",
      "exact deduplicates dropped:27392\n",
      "observations after exact deduplication:254161\n",
      "Observations with missing abstracts after exact deduplication: 11208\n",
      "Observations with missing titles after exact deduplication: 339\n",
      "Observations with missing titles after exact deduplication: 4957\n",
      "Observations with missing titles after exact deduplication: 0\n",
      "                                      id  \\\n",
      "2       https://openalex.org/W2541041155   \n",
      "10      https://openalex.org/W2749538309   \n",
      "14      https://openalex.org/W2275251747   \n",
      "39      https://openalex.org/W1596891619   \n",
      "55      https://openalex.org/W4378596524   \n",
      "...                                  ...   \n",
      "274235  https://openalex.org/W4234346229   \n",
      "274494  https://openalex.org/W4241822235   \n",
      "275274  https://openalex.org/W2793045527   \n",
      "280902  https://openalex.org/W4200003656   \n",
      "280931  https://openalex.org/W2000072941   \n",
      "\n",
      "                                                    title abstract  \\\n",
      "2       Framing as social uncertainty in building urba...     None   \n",
      "10      Urban Micro-climate Simulation Based on 3D Mod...     None   \n",
      "14      Ways of Knowing, Ways of Life. Environment, Ed...     None   \n",
      "39      HYDROLOGIC IMPACT OF CLIMATE CHANGE IN SEMI-UR...     None   \n",
      "55      Investigation of the exterior skin proportions...     None   \n",
      "...                                                   ...      ...   \n",
      "274235  Can the Bonding Social Capital Be Used to Miti...     None   \n",
      "274494  Can the Bonding Social Capital Be Used to Miti...     None   \n",
      "275274  “Sandy’s Remains/#bottlesNbones: Tracking 130 ...     None   \n",
      "280902  The Impact of Climatic Changes on Sustainable ...     None   \n",
      "280931  Urban Climate Change Crossroads edited by Rich...     None   \n",
      "\n",
      "                                                  authors  publication_year  \\\n",
      "2                                          J.A. Wardekker              2016   \n",
      "10      Zheng Zihao, Yingbiao Chen, Qinglan Qian, Li Y...              2016   \n",
      "14                                        Miriam Ladstein              2015   \n",
      "39                                     Shamarokh Arjumand              2012   \n",
      "55      R. Mesgaran Kermani, Seyed Majid Mofidi Shemir...              2022   \n",
      "...                                                   ...               ...   \n",
      "274235                                               None              2018   \n",
      "274494                                               None              2018   \n",
      "275274                                       Jamie Bianco              2013   \n",
      "280902           Muhammad Amir Aizat Khamis, Eman Khalefa              2021   \n",
      "280931                                       Anna Nowicki              2013   \n",
      "\n",
      "        cited_by_count                                   cited_by_api_url  \\\n",
      "2                    1  https://api.openalex.org/works?filter=cites:W2...   \n",
      "10                   1  https://api.openalex.org/works?filter=cites:W2...   \n",
      "14                   1  https://api.openalex.org/works?filter=cites:W2...   \n",
      "39                   1  https://api.openalex.org/works?filter=cites:W1...   \n",
      "55                   1  https://api.openalex.org/works?filter=cites:W4...   \n",
      "...                ...                                                ...   \n",
      "274235               0  https://api.openalex.org/works?filter=cites:W4...   \n",
      "274494               0  https://api.openalex.org/works?filter=cites:W4...   \n",
      "275274               0  https://api.openalex.org/works?filter=cites:W2...   \n",
      "280902               0  https://api.openalex.org/works?filter=cites:W4...   \n",
      "280931               0  https://api.openalex.org/works?filter=cites:W2...   \n",
      "\n",
      "                                                   doi  type  \n",
      "2                                                 None  None  \n",
      "10                                                None  None  \n",
      "14                                                None  None  \n",
      "39                                                None  None  \n",
      "55          https://doi.org/10.22034/ijhcum.2022.02.08  None  \n",
      "...                                                ...   ...  \n",
      "274235            https://doi.org/10.2139/ssrn.3298318  None  \n",
      "274494            https://doi.org/10.2139/ssrn.3196169  None  \n",
      "275274                                            None  None  \n",
      "280902  https://doi.org/10.21608/sjas.2021.100427.1158  None  \n",
      "280931               https://doi.org/10.1111/aec.12071  None  \n",
      "\n",
      "[11208 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "pre_count = len(combined_df)\n",
    "print(\"observations before exact deduplication:\" +  str())\n",
    "combined_df = combined_df.drop_duplicates(subset=[\"id\"])\n",
    "duplicates_dropped = pre_count-len(combined_df)\n",
    "print(\"exact deduplicates dropped:\" + str(duplicates_dropped))\n",
    "print(\"observations after exact deduplication:\" + str(len(combined_df)))\n",
    "print(\"Observations with missing abstracts after exact deduplication: \" + str(combined_df[\"abstract\"].isna().sum()))\n",
    "print(\"Observations with missing titles after exact deduplication: \" + str(combined_df[\"title\"].isna().sum()))\n",
    "print(\"Observations with missing titles after exact deduplication: \" + str(combined_df[\"authors\"].isna().sum()))\n",
    "print(\"Observations with missing titles after exact deduplication: \" + str(combined_df[\"publication_year\"].isna().sum()))\n",
    "print(combined_df[combined_df[\"abstract\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "505de323-a323-481f-88fd-977b08c4186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing abstracts: 11208\n",
      "Missing titles: 339\n",
      "Missing authors: 4957\n",
      "Missing publication years: 0\n"
     ]
    }
   ],
   "source": [
    "from semhash import SemHash\n",
    "import numpy as np\n",
    "\n",
    "filtered_df = combined_df.copy()\n",
    "\n",
    "cols_to_string = [\"title\", \"authors\", \"abstract\", \"publication_year\"]\n",
    "\n",
    "for col in cols_to_string:\n",
    "    filtered_df[col] = filtered_df[col].astype(str)\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "records = filtered_df[[\"id\", \"title\", \"abstract\", \"authors\", \"publication_year\"]].to_dict(orient=\"records\")\n",
    "\n",
    "# Now count missing values\n",
    "missing_titles = sum(1 for rec in records if rec[\"title\"] == \"None\")\n",
    "missing_abstracts = sum(1 for rec in records if rec[\"abstract\"] == \"None\")\n",
    "missing_authors = sum(1 for rec in records if rec[\"authors\"] == \"None\")\n",
    "missing_pub_years = sum(1 for rec in records if rec[\"publication_year\"] == \"None\")\n",
    "\n",
    "print(f\"Missing abstracts: {missing_abstracts}\")\n",
    "print(f\"Missing titles: {missing_titles}\")\n",
    "print(f\"Missing authors: {missing_authors}\")\n",
    "print(f\"Missing publication years: {missing_pub_years}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba5a4e-9440-4a04-bf5a-5522c772d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "for rec in records:\n",
    "    rec['text'] = f\"{rec['title']} {rec['abstract']}\"\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize SemHash with that model\n",
    "semhash = SemHash.from_records(records=records, columns=[\"text\"], model=model)\n",
    "\n",
    "threshold = 0.88\n",
    "dedup_result = semhash.self_deduplicate(threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fbaac8-7dd8-4cd0-a169-63f7858f868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class so that I can call attributes \n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DuplicateRecord:\n",
    "    record: dict\n",
    "    exact: bool\n",
    "    duplicates: list\n",
    "    \n",
    "# A list to collect records with no valid duplicates\n",
    "non_matching_records = []\n",
    "\n",
    "# A new list to store filtered duplicates that meet the match criteria\n",
    "filtered_duplicates = []\n",
    "\n",
    "# Iterate through all duplicate records\n",
    "for dup in dedup_result.duplicates:\n",
    "    original = dup.record\n",
    "    original_year = int(float(original.get(\"publication_year\")))\n",
    "\n",
    "    # List to hold valid matches for this original record\n",
    "    valid_matches = []\n",
    "\n",
    "    for rec, score in dup.duplicates:\n",
    "        duplicate_year = int(float(rec.get(\"publication_year\")))\n",
    "\n",
    "        has_abstract = rec.get('abstract_en') and rec['abstract_en'].strip().lower() != \"none\"\n",
    "        has_title = rec.get('title') and rec['title'].strip().lower() != \"none\"\n",
    "        # has_authors = rec.get('authors') and rec['authors'].strip().lower() != \"none\"\n",
    "        \n",
    "        close_years = abs(duplicate_year - original_year) <= 1\n",
    "        match = False\n",
    "\n",
    "        # Apply matching rules\n",
    "        if close_years and has_abstract and has_title and score > 0.9:\n",
    "            match = True\n",
    "        elif close_years and has_abstract and not has_title and score > 0.9:\n",
    "            match = True\n",
    "        elif close_years and not has_abstract and has_title and score > 0.92:\n",
    "            match = True\n",
    "        # has to be specified for completeness but there are no applicable observations, here\n",
    "        elif close_years and not has_abstract and not has_title and score > 0.99:\n",
    "            match = True\n",
    "\n",
    "        if match:\n",
    "            valid_matches.append((rec, score))\n",
    "\n",
    "    # Store record only if it has valid matches\n",
    "    if valid_matches:\n",
    "        filtered_duplicates.append(\n",
    "            DuplicateRecord(record=original, duplicates=valid_matches, exact=False)\n",
    "        )\n",
    "    else:\n",
    "        non_matching_records.append(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2bb20-5ecb-4467-9f67-de31bdaabbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the numbers are right:\n",
    "print(f\"Duplicates based on more restrictive criteria with individual thresholds: {sum(1 for record in filtered_duplicates)}\")\n",
    "print(f\"Number of duplicates that are likely falsely detected as such: {len(non_matching_records)}\")\n",
    "print(f\"Restricted duplicates and those duplicates that do not belong to the restricted duplicates but had initially been predicted as such = {sum(1 for record in filtered_duplicates) + len(non_matching_records)} must be equal to the initial number of duplicates = {len(dedup_result.duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e44dff-9257-4df9-85a7-92c5cf89a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add false positive duplicates to the deduplicated list\n",
    "filtered_deduplicated = dedup_result.deduplicated.copy()\n",
    "filtered_deduplicated.extend(non_matching_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043912cd-5da1-430e-a127-41cffaa77d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform to dfs \n",
    "# SemHash: duplicates and the deduplicated records from the semhashing as dataframes\n",
    "deduplicated_hash_df = pd.DataFrame(dedup_result.deduplicated)\n",
    "dup_records = [dup.record for dup in dedup_result.duplicates]\n",
    "duplicates_hash_df = pd.DataFrame(dup_records)\n",
    "# customized additional filtering:\n",
    "filtered_duplicates_df = pd.DataFrame(filtered_duplicates)\n",
    "filtered_deduplicated_df = pd.DataFrame(filtered_deduplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439afa4-93d7-4333-9951-a570451cffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all those ids that are insided \n",
    "print(\"----------SemHash numbers-------------------------------------------------------------------------\")\n",
    "print(f\"SemHash deduplicated: {len(deduplicated_hash_df)}\")\n",
    "print(f\"SemHash duplicates: {len(duplicates_hash_df)}\")\n",
    "print(\"----------Restricted filtering numbers------------------------------------------------------------\")\n",
    "print(f\"Restricted deduplicated: {len(filtered_duplicates)}\")\n",
    "print(f\"Restricted duplicates: {len(filtered_deduplicated)}\")\n",
    "print(\"----------Additional------------------------------------------------------------------------------\")\n",
    "print(f\"SemHash duplicates that are no duplicates according to restricted criteria: {len(duplicates_hash_df)- len(filtered_duplicates)}\")\n",
    "print(f\"All wrongly assigned SemHash duplicates are correctly reasssigned: {len(filtered_deduplicated)-len(deduplicated_hash_df) == len(duplicates_hash_df) - len(filtered_duplicates)}\")\n",
    "print(f\"Final percentage of removed: {(len(filtered_duplicates_df)/ (len(deduplicated_hash_df) + len(duplicates_hash_df))):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9effe-6a8e-4f0e-816a-9d672c1b4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates_by_na_fields(duplicates, require_abstract=True, require_title=True):\n",
    "    \"\"\"\n",
    "    Filters and prints the 10 duplicate groups with the lowest average score,\n",
    "    where the original record meets the specified NA conditions.\n",
    "    \n",
    "    Args:\n",
    "        duplicates (list): List of DuplicateRecord objects.\n",
    "        require_abstract (bool): If True, abstract must NOT be NA.\n",
    "        require_title (bool): If True, title must NOT be NA.\n",
    "        require_authors (bool): If True, authors must NOT be NA.\n",
    "    \"\"\"\n",
    "    if not duplicates:\n",
    "        print(\"No duplicates to check.\")\n",
    "        return\n",
    "\n",
    "    filtered = []\n",
    "\n",
    "    for dup in duplicates:\n",
    "        has_abstract = dup.record.get('abstract_en') and dup.record['abstract_en'].strip().lower() != \"none\"\n",
    "        has_title = dup.record.get('title') and dup.record['title'].strip().lower() != \"none\"\n",
    "        # has_authors = dup.record.get('authors') and dup.record['authors'].strip().lower() != \"none\"\n",
    "\n",
    "        condition = (has_abstract == require_abstract) and (has_title == require_title)\n",
    "\n",
    "        if condition and dup.duplicates:\n",
    "            avg_score = sum(score for _, score in dup.duplicates) / len(dup.duplicates)\n",
    "            filtered.append((avg_score, dup))\n",
    "\n",
    "    if not filtered:\n",
    "        print(\"\\nNo duplicates found based on the specified NA conditions.\")\n",
    "        return\n",
    "\n",
    "    # Sort by average score and take the 10 lowest\n",
    "    filtered.sort(key=lambda x: x[0])\n",
    "    filtered = filtered[:10]\n",
    "\n",
    "    for avg_score, dup in filtered:\n",
    "        print(\"\\n=== DUPLICATE RECORD ===\")\n",
    "        print(f\"Original Title : {dup.record.get('title', '')[:100]}\")\n",
    "        print(f\"Authors        : {dup.record.get('authors', '')[:100]}\")\n",
    "        print(f\"Abstract       : {dup.record.get('abstract_en', '')[:100]}\")\n",
    "        print(f\"Year           : {str(dup.record.get('publication_year', ''))[:100]}\")\n",
    "        print(f\"Exact Match    : {dup.exact}\")\n",
    "        print(f\"Avg. Score     : {avg_score:.3f}\")\n",
    "        print(\"Duplicates:\")\n",
    "        for rec, score in dup.duplicates:\n",
    "            print(f\"  - Title   : {rec.get('title', '')[:100]}\")\n",
    "            print(f\"    Authors : {rec.get('authors', '')[:100]}\")\n",
    "            print(f\"    Abstract: {rec.get('abstract_en', '')[:100]}\")\n",
    "            print(f\"    Year    : {str(rec.get('publication_year', ''))[:100]}\")\n",
    "            print(f\"    Score   : {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a1f9c-c4ff-47c5-a580-c455ca15c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_duplicates_by_na_fields(filtered_duplicates, require_abstract=True, require_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfef452-70f4-4069-a460-36ecc0071763",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_duplicates_by_na_fields(filtered_duplicates, require_abstract=True, require_title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668ef0a-f47c-4d56-af78-34d5922c4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_duplicates_by_na_fields(filtered_duplicates, require_abstract=True, require_title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe1827-9066-4bdb-9825-09fc76165745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "os.chdir(dir_path)\n",
    "\n",
    "chunk_size = 20000\n",
    "total_records = len(filtered_deduplicated_df)\n",
    "num_chunks = math.ceil(total_records / chunk_size)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, total_records)\n",
    "    chunk = filtered_deduplicated_df.iloc[start:end]\n",
    "    \n",
    "    filename = f\"data/OpenAlex/05_deduplicated/city_works_df_NA_abstr_added_dedup_{i+1}.parquet\"\n",
    "    chunk.to_parquet(filename)\n",
    "    print(f\"Saved {filename} with records {start} to {end-1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
